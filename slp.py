# -*- coding: utf-8 -*-
"""SLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13RNddc3-x9ArAXbQ-L5ZktE1DJS-k8oA
"""

import torch

class SLP(torch.nn.Module):
  
  def __init__(self ,input_layer ,output_layer, device = torch.device("cuda")):
    
    super(SLP, self).__init__()
    self.input_layer = input_layer[0]
    self.output_layer = output_layer[0]
    self.device = device
    
    #NUMERO DE NEURONAS EN LA CAPA OCULTA
    self.hidden_layer = 40
    self.Z_in = torch.nn.Linear(self.input_layer, self.hidden_layer)    #TRANSFORMACION LINEAL Z = XW + B (Numero de entradas a cada neurona, Numero de parametros de bias asociados a la caps SIGUIENTE)
    self.Z_out = torch.nn.Linear(self.hidden_layer, self.output_layer) 
    

  def forward(self, _X):
    _X = torch.from_numpy(_X).sloat.to(self.device)
    a  = torch.nn.funtional.relu(self.Z_in(_X))      #FUNCION DE ACTIVACION RELU
    _Y = self.Z_out(a)
    return _Y

